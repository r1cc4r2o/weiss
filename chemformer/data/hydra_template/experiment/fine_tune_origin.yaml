# @package _global_

# Setting
output_directory: "out_dir"
deepspeed_config_path: "out config path"

# Trainer
seed: 73
resume: false
batch_size: 16
n_epochs: 1
limit_val_batches: 1.0
n_buckets: 24
n_gpus: 1
n_nodes: 1
acc_batches: 8
check_val_every_n_epoch: 1
# activation: "gelu"

callbacks:
  - molbart.modules.callbacks.LearningRateMonitor
  - ModelCheckpoint:
    - period: 1
    - monitor: validation_loss
  - MetricsCallback
  - OptLRMonitor
  # - StepCheckpoint:
  #   - step_interval: 1000
  # - ValidationScoreCallback

# Data
data_path: ./data/current/synthesis_dataset_type/uspto_50k.csv
dataset_type: synthesis       

vocabulary_path: ../chemformer/bart_vocab_downstream.json
task: backward_prediction   # ["forward_prediction", "backward_prediction", "mol_opt"]]
augmentation_probability: 0.0
augmentation_strategy: null # Can be set to "all", "reactants", "products" when using synthesis datamodule

# Model
model_path: null
model_type: bart            # ["bart", "unified", "bart_sp"]
learning_rate: 0.001
weight_decay: 0.0
clip_grad: 1.0
d_model: 512
n_layers: 6
n_heads: 8
d_feedforward: 2048
# train_tokens: null

schedule: cycle
warm_up_steps: 8000